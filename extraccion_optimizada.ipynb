{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción Optimizada de Atributos con Gemini\n",
    "\n",
    "Este notebook extrae atributos de productos a partir de imágenes usando Google Gemini API.\n",
    "\n",
    "## Características:\n",
    "- Extracción desde CSV\n",
    "- Guardado automático en CSV\n",
    "- Reintentos automáticos con backoff exponencial\n",
    "- Reanudación automática desde último punto\n",
    "- Barra de progreso visual\n",
    "- Logging detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/franciscomath/matriz-atributos/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai pandas tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'genai' from 'google' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'genai' from 'google' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3316172160.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    uv pip install google-genai\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "uv pip install google-genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('extraccion_atributos.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configuración de rutas\n",
    "class Config:\n",
    "    \"\"\"Configuración centralizada del proyecto\"\"\"\n",
    "    \n",
    "    # Rutas\n",
    "    PROMPT_FILE = Path('prompt_api.txt')\n",
    "    IMAGE_DIRECTORY = Path('images')  # Carpeta con las imágenes\n",
    "    INPUT_CSV = Path('productos.csv')  # CSV de entrada con columnas: id, image, metadatos\n",
    "    OUTPUT_CSV = Path('productos_con_atributos.csv')  # CSV de salida\n",
    "    \n",
    "    # API Configuration\n",
    "    GEMINI_MODEL = 'gemini-2.0-flash-exp'  # Modelo más reciente y rápido\n",
    "    MAX_RETRIES = 5\n",
    "    BASE_DELAY = 5  # segundos\n",
    "    RATE_LIMIT_DELAY = 1.5  # segundos entre llamadas\n",
    "    \n",
    "    # Columnas esperadas en el CSV\n",
    "    ID_COLUMN = 'id'\n",
    "    IMAGE_COLUMN = 'image'\n",
    "    ATTRIBUTES_COLUMN = 'gemini_attributes'\n",
    "\n",
    "config = Config()\n",
    "logger.info(f\"Configuración cargada: {config.__dict__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar cliente de Gemini\n",
    "try:\n",
    "    client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "    logger.info(\"✅ Cliente de Gemini inicializado correctamente\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error al inicializar cliente de Gemini: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_path: Path) -> str:\n",
    "    \"\"\"Carga el texto del prompt desde un archivo.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            prompt = f.read().strip()\n",
    "        logger.info(f\"Prompt cargado desde {file_path} ({len(prompt)} caracteres)\")\n",
    "        return prompt\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "            prompt = f.read().strip()\n",
    "        logger.warning(f\"Prompt cargado con encoding latin-1\")\n",
    "        return prompt\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar prompt: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_mime_type(image_path: Path) -> str:\n",
    "    \"\"\"Determina el tipo MIME de la imagen.\"\"\"\n",
    "    extension = image_path.suffix.lower()\n",
    "    mime_types = {\n",
    "        '.jpg': 'image/jpeg',\n",
    "        '.jpeg': 'image/jpeg',\n",
    "        '.png': 'image/png',\n",
    "        '.gif': 'image/gif',\n",
    "        '.webp': 'image/webp'\n",
    "    }\n",
    "    return mime_types.get(extension, 'image/jpeg')\n",
    "\n",
    "\n",
    "def process_image_with_gemini(\n",
    "    image_path: Path,\n",
    "    prompt_text: str,\n",
    "    max_retries: int = config.MAX_RETRIES,\n",
    "    base_delay: int = config.BASE_DELAY\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Procesa una imagen con Gemini API y retorna los atributos extraídos.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Ruta a la imagen\n",
    "        prompt_text: Texto del prompt\n",
    "        max_retries: Número máximo de reintentos\n",
    "        base_delay: Delay base para backoff exponencial\n",
    "        \n",
    "    Returns:\n",
    "        String con los atributos extraídos o mensaje de error\n",
    "    \"\"\"\n",
    "    if not image_path.exists():\n",
    "        return f\"ERROR_IMAGEN: Archivo no encontrado en {image_path}\"\n",
    "    \n",
    "    # Leer imagen\n",
    "    try:\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_LECTURA: {str(e)}\"\n",
    "    \n",
    "    mime_type = get_mime_type(image_path)\n",
    "    \n",
    "    # Preparar contenido para Gemini\n",
    "    contents = [\n",
    "        types.Part.from_bytes(data=image_bytes, mime_type=mime_type),\n",
    "        types.Part.from_text(text=prompt_text)\n",
    "    ]\n",
    "    \n",
    "    # Reintentos con backoff exponencial\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=config.GEMINI_MODEL,\n",
    "                contents=contents\n",
    "            )\n",
    "            return response.text.strip().replace('\\n', ' ')\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            logger.warning(f\"Intento {attempt + 1}/{max_retries} falló: {error_message}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                sleep_time = base_delay * (2 ** attempt)\n",
    "                logger.info(f\"Esperando {sleep_time}s antes de reintentar...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                return f\"ERROR_API_FATAL: {error_message}\"\n",
    "    \n",
    "    return \"ERROR_INESPERADO: Bucle de reintento fallido\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_dataframe(input_csv: Path) -> pd.DataFrame:\n",
    "    \"\"\"Carga el CSV de entrada o crea uno vacío si no existe.\"\"\"\n",
    "    if input_csv.exists():\n",
    "        df = pd.read_csv(input_csv)\n",
    "        logger.info(f\"CSV cargado: {len(df)} registros\")\n",
    "        \n",
    "        # Asegurar que exista la columna de atributos\n",
    "        if config.ATTRIBUTES_COLUMN not in df.columns:\n",
    "            df[config.ATTRIBUTES_COLUMN] = ''\n",
    "            logger.info(f\"Columna '{config.ATTRIBUTES_COLUMN}' creada\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(f\"Archivo {input_csv} no encontrado. Creando DataFrame vacío.\")\n",
    "        return pd.DataFrame(columns=[config.ID_COLUMN, config.IMAGE_COLUMN, config.ATTRIBUTES_COLUMN])\n",
    "\n",
    "\n",
    "def save_checkpoint(df: pd.DataFrame, output_csv: Path) -> None:\n",
    "    \"\"\"Guarda el DataFrame en CSV.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        logger.info(f\"✅ Checkpoint guardado en {output_csv}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error al guardar checkpoint: {e}\")\n",
    "\n",
    "\n",
    "def should_process_row(row: pd.Series) -> bool:\n",
    "    \"\"\"Determina si una fila debe ser procesada.\"\"\"\n",
    "    attributes_value = str(row.get(config.ATTRIBUTES_COLUMN, '')).strip()\n",
    "    \n",
    "    # Procesar si está vacío o es un error fatal que queremos reintentar\n",
    "    if not attributes_value:\n",
    "        return True\n",
    "    \n",
    "    # No reprocesar si ya tiene atributos válidos\n",
    "    if not attributes_value.startswith('ERROR'):\n",
    "        return False\n",
    "    \n",
    "    # Reprocesar solo errores fatales\n",
    "    if 'ERROR_API_FATAL' in attributes_value:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proceso Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(\n",
    "    input_csv: Path = config.INPUT_CSV,\n",
    "    output_csv: Path = config.OUTPUT_CSV,\n",
    "    prompt_file: Path = config.PROMPT_FILE,\n",
    "    image_dir: Path = config.IMAGE_DIRECTORY,\n",
    "    save_every: int = 1  # Guardar después de cada procesamiento\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso de extracción de atributos.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Ruta al CSV de entrada\n",
    "        output_csv: Ruta al CSV de salida\n",
    "        prompt_file: Ruta al archivo de prompt\n",
    "        image_dir: Directorio con las imágenes\n",
    "        save_every: Guardar cada N procesados (default: 1)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con los atributos extraídos\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"INICIANDO EXTRACCIÓN DE ATRIBUTOS\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    # Cargar prompt\n",
    "    prompt = load_prompt(prompt_file)\n",
    "    \n",
    "    # Cargar o crear DataFrame\n",
    "    df = load_or_create_dataframe(input_csv)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        logger.warning(\"⚠️ No hay datos para procesar\")\n",
    "        return df\n",
    "    \n",
    "    # Filtrar filas a procesar\n",
    "    rows_to_process = df.apply(should_process_row, axis=1)\n",
    "    total_to_process = rows_to_process.sum()\n",
    "    \n",
    "    logger.info(f\"Total de productos: {len(df)}\")\n",
    "    logger.info(f\"A procesar: {total_to_process}\")\n",
    "    logger.info(f\"Ya procesados: {len(df) - total_to_process}\")\n",
    "    \n",
    "    if total_to_process == 0:\n",
    "        logger.info(\"✨ Todos los productos ya están procesados\")\n",
    "        return df\n",
    "    \n",
    "    # Procesar cada fila\n",
    "    processed_count = 0\n",
    "    \n",
    "    with tqdm(total=total_to_process, desc=\"Extrayendo atributos\") as pbar:\n",
    "        for idx, row in df.iterrows():\n",
    "            if not rows_to_process[idx]:\n",
    "                continue\n",
    "            \n",
    "            product_id = row.get(config.ID_COLUMN, idx)\n",
    "            image_filename = row.get(config.IMAGE_COLUMN, '')\n",
    "            \n",
    "            if not image_filename:\n",
    "                logger.warning(f\"Fila {idx}: Sin nombre de imagen\")\n",
    "                df.at[idx, config.ATTRIBUTES_COLUMN] = \"ERROR_SIN_IMAGEN\"\n",
    "                continue\n",
    "            \n",
    "            image_path = image_dir / image_filename\n",
    "            \n",
    "            logger.info(f\"Procesando {product_id}: {image_filename}\")\n",
    "            \n",
    "            # Procesar con Gemini\n",
    "            attributes = process_image_with_gemini(image_path, prompt)\n",
    "            \n",
    "            # Guardar resultado\n",
    "            df.at[idx, config.ATTRIBUTES_COLUMN] = attributes\n",
    "            \n",
    "            # Log resultado\n",
    "            if attributes.startswith(\"ERROR\"):\n",
    "                logger.error(f\"❌ {product_id}: {attributes[:100]}\")\n",
    "            else:\n",
    "                logger.info(f\"✅ {product_id}: {attributes[:80]}...\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Guardar checkpoint\n",
    "            if processed_count % save_every == 0:\n",
    "                save_checkpoint(df, output_csv)\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(config.RATE_LIMIT_DELAY)\n",
    "    \n",
    "    # Guardar resultado final\n",
    "    save_checkpoint(df, output_csv)\n",
    "    \n",
    "    # Estadísticas finales\n",
    "    successful = len(df[~df[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)])\n",
    "    errors = len(df[df[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)])\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"PROCESO COMPLETADO\")\n",
    "    logger.info(f\"✅ Exitosos: {successful}\")\n",
    "    logger.info(f\"❌ Errores: {errors}\")\n",
    "    logger.info(f\"📁 Guardado en: {output_csv}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el proceso de extracción\n",
    "df_result = run_extraction()\n",
    "\n",
    "# Mostrar primeros resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRIMEROS 5 RESULTADOS:\")\n",
    "print(\"=\"*60)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas de procesamiento\n",
    "print(\"\\n📊 ESTADÍSTICAS DE PROCESAMIENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_result)\n",
    "with_attributes = len(df_result[df_result[config.ATTRIBUTES_COLUMN].str.len() > 0])\n",
    "errors = len(df_result[df_result[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)])\n",
    "successful = with_attributes - errors\n",
    "pending = total - with_attributes\n",
    "\n",
    "print(f\"Total de productos: {total}\")\n",
    "print(f\"✅ Procesados exitosamente: {successful} ({successful/total*100:.1f}%)\")\n",
    "print(f\"❌ Con errores: {errors} ({errors/total*100:.1f}%)\")\n",
    "print(f\"⏳ Pendientes: {pending} ({pending/total*100:.1f}%)\")\n",
    "\n",
    "# Tipos de errores\n",
    "if errors > 0:\n",
    "    print(\"\\n🔍 TIPOS DE ERRORES:\")\n",
    "    error_df = df_result[df_result[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)]\n",
    "    error_types = error_df[config.ATTRIBUTES_COLUMN].str.split(':', expand=True)[0].value_counts()\n",
    "    for error_type, count in error_types.items():\n",
    "        print(f\"  {error_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exportar Resultados Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar solo productos procesados exitosamente\n",
    "df_clean = df_result[~df_result[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)]\n",
    "df_clean = df_clean[df_clean[config.ATTRIBUTES_COLUMN].str.len() > 0]\n",
    "\n",
    "output_clean = Path('productos_limpios.csv')\n",
    "df_clean.to_csv(output_clean, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n✅ Resultados limpios exportados a: {output_clean}\")\n",
    "print(f\"Total de registros limpios: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Utilidades Adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para reprocesar solo errores\n",
    "def reprocess_errors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reprocesa solo los registros con errores.\"\"\"\n",
    "    error_mask = df[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)\n",
    "    df.loc[error_mask, config.ATTRIBUTES_COLUMN] = ''  # Limpiar errores\n",
    "    return run_extraction()\n",
    "\n",
    "# Descomentar para reprocesar errores\n",
    "# df_result = reprocess_errors(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para verificar imágenes faltantes\n",
    "def check_missing_images(df: pd.DataFrame, image_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Verifica qué imágenes no se encuentran en el directorio.\"\"\"\n",
    "    missing = []\n",
    "    for idx, row in df.iterrows():\n",
    "        image_filename = row.get(config.IMAGE_COLUMN, '')\n",
    "        if image_filename:\n",
    "            image_path = image_dir / image_filename\n",
    "            if not image_path.exists():\n",
    "                missing.append({\n",
    "                    'id': row.get(config.ID_COLUMN, idx),\n",
    "                    'image': image_filename\n",
    "                })\n",
    "    \n",
    "    missing_df = pd.DataFrame(missing)\n",
    "    print(f\"\\n🔍 Imágenes faltantes: {len(missing)}\")\n",
    "    return missing_df\n",
    "\n",
    "# Descomentar para verificar imágenes faltantes\n",
    "# missing_images = check_missing_images(df_result, config.IMAGE_DIRECTORY)\n",
    "# missing_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matriz-atributos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
