{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracci√≥n Optimizada de Atributos con Gemini\n",
    "\n",
    "Este notebook extrae atributos de productos a partir de im√°genes usando Google Gemini API.\n",
    "\n",
    "## Caracter√≠sticas:\n",
    "- Extracci√≥n desde CSV\n",
    "- Guardado autom√°tico en CSV\n",
    "- Reintentos autom√°ticos con backoff exponencial\n",
    "- Reanudaci√≥n autom√°tica desde √∫ltimo punto\n",
    "- Barra de progreso visual\n",
    "- Logging detallado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/franciscomath/matriz-atributos/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai pandas tqdm python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'genai' from 'google' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'genai' from 'google' (unknown location)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3316172160.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    uv pip install google-genai\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "uv pip install google-genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('extraccion_atributos.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Configuraci√≥n de rutas\n",
    "class Config:\n",
    "    \"\"\"Configuraci√≥n centralizada del proyecto\"\"\"\n",
    "    \n",
    "    # Rutas\n",
    "    PROMPT_FILE = Path('prompt_api.txt')\n",
    "    IMAGE_DIRECTORY = Path('images')  # Carpeta con las im√°genes\n",
    "    INPUT_CSV = Path('productos.csv')  # CSV de entrada con columnas: id, image, metadatos\n",
    "    OUTPUT_CSV = Path('productos_con_atributos.csv')  # CSV de salida\n",
    "    \n",
    "    # API Configuration\n",
    "    GEMINI_MODEL = 'gemini-2.0-flash-exp'  # Modelo m√°s reciente y r√°pido\n",
    "    MAX_RETRIES = 5\n",
    "    BASE_DELAY = 5  # segundos\n",
    "    RATE_LIMIT_DELAY = 1.5  # segundos entre llamadas\n",
    "    \n",
    "    # Columnas esperadas en el CSV\n",
    "    ID_COLUMN = 'id'\n",
    "    IMAGE_COLUMN = 'image'\n",
    "    ATTRIBUTES_COLUMN = 'gemini_attributes'\n",
    "\n",
    "config = Config()\n",
    "logger.info(f\"Configuraci√≥n cargada: {config.__dict__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar cliente de Gemini\n",
    "try:\n",
    "    client = genai.Client(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "    logger.info(\"‚úÖ Cliente de Gemini inicializado correctamente\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Error al inicializar cliente de Gemini: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt(file_path: Path) -> str:\n",
    "    \"\"\"Carga el texto del prompt desde un archivo.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            prompt = f.read().strip()\n",
    "        logger.info(f\"Prompt cargado desde {file_path} ({len(prompt)} caracteres)\")\n",
    "        return prompt\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "            prompt = f.read().strip()\n",
    "        logger.warning(f\"Prompt cargado con encoding latin-1\")\n",
    "        return prompt\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar prompt: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_mime_type(image_path: Path) -> str:\n",
    "    \"\"\"Determina el tipo MIME de la imagen.\"\"\"\n",
    "    extension = image_path.suffix.lower()\n",
    "    mime_types = {\n",
    "        '.jpg': 'image/jpeg',\n",
    "        '.jpeg': 'image/jpeg',\n",
    "        '.png': 'image/png',\n",
    "        '.gif': 'image/gif',\n",
    "        '.webp': 'image/webp'\n",
    "    }\n",
    "    return mime_types.get(extension, 'image/jpeg')\n",
    "\n",
    "\n",
    "def process_image_with_gemini(\n",
    "    image_path: Path,\n",
    "    prompt_text: str,\n",
    "    max_retries: int = config.MAX_RETRIES,\n",
    "    base_delay: int = config.BASE_DELAY\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Procesa una imagen con Gemini API y retorna los atributos extra√≠dos.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Ruta a la imagen\n",
    "        prompt_text: Texto del prompt\n",
    "        max_retries: N√∫mero m√°ximo de reintentos\n",
    "        base_delay: Delay base para backoff exponencial\n",
    "        \n",
    "    Returns:\n",
    "        String con los atributos extra√≠dos o mensaje de error\n",
    "    \"\"\"\n",
    "    if not image_path.exists():\n",
    "        return f\"ERROR_IMAGEN: Archivo no encontrado en {image_path}\"\n",
    "    \n",
    "    # Leer imagen\n",
    "    try:\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_bytes = f.read()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR_LECTURA: {str(e)}\"\n",
    "    \n",
    "    mime_type = get_mime_type(image_path)\n",
    "    \n",
    "    # Preparar contenido para Gemini\n",
    "    contents = [\n",
    "        types.Part.from_bytes(data=image_bytes, mime_type=mime_type),\n",
    "        types.Part.from_text(text=prompt_text)\n",
    "    ]\n",
    "    \n",
    "    # Reintentos con backoff exponencial\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=config.GEMINI_MODEL,\n",
    "                contents=contents\n",
    "            )\n",
    "            return response.text.strip().replace('\\n', ' ')\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            logger.warning(f\"Intento {attempt + 1}/{max_retries} fall√≥: {error_message}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                sleep_time = base_delay * (2 ** attempt)\n",
    "                logger.info(f\"Esperando {sleep_time}s antes de reintentar...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                return f\"ERROR_API_FATAL: {error_message}\"\n",
    "    \n",
    "    return \"ERROR_INESPERADO: Bucle de reintento fallido\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_dataframe(input_csv: Path) -> pd.DataFrame:\n",
    "    \"\"\"Carga el CSV de entrada o crea uno vac√≠o si no existe.\"\"\"\n",
    "    if input_csv.exists():\n",
    "        df = pd.read_csv(input_csv)\n",
    "        logger.info(f\"CSV cargado: {len(df)} registros\")\n",
    "        \n",
    "        # Asegurar que exista la columna de atributos\n",
    "        if config.ATTRIBUTES_COLUMN not in df.columns:\n",
    "            df[config.ATTRIBUTES_COLUMN] = ''\n",
    "            logger.info(f\"Columna '{config.ATTRIBUTES_COLUMN}' creada\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        logger.warning(f\"Archivo {input_csv} no encontrado. Creando DataFrame vac√≠o.\")\n",
    "        return pd.DataFrame(columns=[config.ID_COLUMN, config.IMAGE_COLUMN, config.ATTRIBUTES_COLUMN])\n",
    "\n",
    "\n",
    "def save_checkpoint(df: pd.DataFrame, output_csv: Path) -> None:\n",
    "    \"\"\"Guarda el DataFrame en CSV.\"\"\"\n",
    "    try:\n",
    "        df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        logger.info(f\"‚úÖ Checkpoint guardado en {output_csv}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error al guardar checkpoint: {e}\")\n",
    "\n",
    "\n",
    "def should_process_row(row: pd.Series) -> bool:\n",
    "    \"\"\"Determina si una fila debe ser procesada.\"\"\"\n",
    "    attributes_value = str(row.get(config.ATTRIBUTES_COLUMN, '')).strip()\n",
    "    \n",
    "    # Procesar si est√° vac√≠o o es un error fatal que queremos reintentar\n",
    "    if not attributes_value:\n",
    "        return True\n",
    "    \n",
    "    # No reprocesar si ya tiene atributos v√°lidos\n",
    "    if not attributes_value.startswith('ERROR'):\n",
    "        return False\n",
    "    \n",
    "    # Reprocesar solo errores fatales\n",
    "    if 'ERROR_API_FATAL' in attributes_value:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proceso Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction(\n",
    "    input_csv: Path = config.INPUT_CSV,\n",
    "    output_csv: Path = config.OUTPUT_CSV,\n",
    "    prompt_file: Path = config.PROMPT_FILE,\n",
    "    image_dir: Path = config.IMAGE_DIRECTORY,\n",
    "    save_every: int = 1  # Guardar despu√©s de cada procesamiento\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso de extracci√≥n de atributos.\n",
    "    \n",
    "    Args:\n",
    "        input_csv: Ruta al CSV de entrada\n",
    "        output_csv: Ruta al CSV de salida\n",
    "        prompt_file: Ruta al archivo de prompt\n",
    "        image_dir: Directorio con las im√°genes\n",
    "        save_every: Guardar cada N procesados (default: 1)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con los atributos extra√≠dos\n",
    "    \"\"\"\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"INICIANDO EXTRACCI√ìN DE ATRIBUTOS\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    # Cargar prompt\n",
    "    prompt = load_prompt(prompt_file)\n",
    "    \n",
    "    # Cargar o crear DataFrame\n",
    "    df = load_or_create_dataframe(input_csv)\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        logger.warning(\"‚ö†Ô∏è No hay datos para procesar\")\n",
    "        return df\n",
    "    \n",
    "    # Filtrar filas a procesar\n",
    "    rows_to_process = df.apply(should_process_row, axis=1)\n",
    "    total_to_process = rows_to_process.sum()\n",
    "    \n",
    "    logger.info(f\"Total de productos: {len(df)}\")\n",
    "    logger.info(f\"A procesar: {total_to_process}\")\n",
    "    logger.info(f\"Ya procesados: {len(df) - total_to_process}\")\n",
    "    \n",
    "    if total_to_process == 0:\n",
    "        logger.info(\"‚ú® Todos los productos ya est√°n procesados\")\n",
    "        return df\n",
    "    \n",
    "    # Procesar cada fila\n",
    "    processed_count = 0\n",
    "    \n",
    "    with tqdm(total=total_to_process, desc=\"Extrayendo atributos\") as pbar:\n",
    "        for idx, row in df.iterrows():\n",
    "            if not rows_to_process[idx]:\n",
    "                continue\n",
    "            \n",
    "            product_id = row.get(config.ID_COLUMN, idx)\n",
    "            image_filename = row.get(config.IMAGE_COLUMN, '')\n",
    "            \n",
    "            if not image_filename:\n",
    "                logger.warning(f\"Fila {idx}: Sin nombre de imagen\")\n",
    "                df.at[idx, config.ATTRIBUTES_COLUMN] = \"ERROR_SIN_IMAGEN\"\n",
    "                continue\n",
    "            \n",
    "            image_path = image_dir / image_filename\n",
    "            \n",
    "            logger.info(f\"Procesando {product_id}: {image_filename}\")\n",
    "            \n",
    "            # Procesar con Gemini\n",
    "            attributes = process_image_with_gemini(image_path, prompt)\n",
    "            \n",
    "            # Guardar resultado\n",
    "            df.at[idx, config.ATTRIBUTES_COLUMN] = attributes\n",
    "            \n",
    "            # Log resultado\n",
    "            if attributes.startswith(\"ERROR\"):\n",
    "                logger.error(f\"‚ùå {product_id}: {attributes[:100]}\")\n",
    "            else:\n",
    "                logger.info(f\"‚úÖ {product_id}: {attributes[:80]}...\")\n",
    "            \n",
    "            processed_count += 1\n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Guardar checkpoint\n",
    "            if processed_count % save_every == 0:\n",
    "                save_checkpoint(df, output_csv)\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(config.RATE_LIMIT_DELAY)\n",
    "    \n",
    "    # Guardar resultado final\n",
    "    save_checkpoint(df, output_csv)\n",
    "    \n",
    "    # Estad√≠sticas finales\n",
    "    successful = len(df[~df[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)])\n",
    "    errors = len(df[df[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)])\n",
    "    \n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"PROCESO COMPLETADO\")\n",
    "    logger.info(f\"‚úÖ Exitosos: {successful}\")\n",
    "    logger.info(f\"‚ùå Errores: {errors}\")\n",
    "    logger.info(f\"üìÅ Guardado en: {output_csv}\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejecuci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el proceso de extracci√≥n\n",
    "df_result = run_extraction()\n",
    "\n",
    "# Mostrar primeros resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRIMEROS 5 RESULTADOS:\")\n",
    "print(\"=\"*60)\n",
    "df_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas de procesamiento\n",
    "print(\"\\nüìä ESTAD√çSTICAS DE PROCESAMIENTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total = len(df_result)\n",
    "with_attributes = len(df_result[df_result[config.ATTRIBUTES_COLUMN].str.len() > 0])\n",
    "errors = len(df_result[df_result[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)])\n",
    "successful = with_attributes - errors\n",
    "pending = total - with_attributes\n",
    "\n",
    "print(f\"Total de productos: {total}\")\n",
    "print(f\"‚úÖ Procesados exitosamente: {successful} ({successful/total*100:.1f}%)\")\n",
    "print(f\"‚ùå Con errores: {errors} ({errors/total*100:.1f}%)\")\n",
    "print(f\"‚è≥ Pendientes: {pending} ({pending/total*100:.1f}%)\")\n",
    "\n",
    "# Tipos de errores\n",
    "if errors > 0:\n",
    "    print(\"\\nüîç TIPOS DE ERRORES:\")\n",
    "    error_df = df_result[df_result[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)]\n",
    "    error_types = error_df[config.ATTRIBUTES_COLUMN].str.split(':', expand=True)[0].value_counts()\n",
    "    for error_type, count in error_types.items():\n",
    "        print(f\"  {error_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exportar Resultados Limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar solo productos procesados exitosamente\n",
    "df_clean = df_result[~df_result[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)]\n",
    "df_clean = df_clean[df_clean[config.ATTRIBUTES_COLUMN].str.len() > 0]\n",
    "\n",
    "output_clean = Path('productos_limpios.csv')\n",
    "df_clean.to_csv(output_clean, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n‚úÖ Resultados limpios exportados a: {output_clean}\")\n",
    "print(f\"Total de registros limpios: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Utilidades Adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para reprocesar solo errores\n",
    "def reprocess_errors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reprocesa solo los registros con errores.\"\"\"\n",
    "    error_mask = df[config.ATTRIBUTES_COLUMN].str.startswith('ERROR', na=False)\n",
    "    df.loc[error_mask, config.ATTRIBUTES_COLUMN] = ''  # Limpiar errores\n",
    "    return run_extraction()\n",
    "\n",
    "# Descomentar para reprocesar errores\n",
    "# df_result = reprocess_errors(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para verificar im√°genes faltantes\n",
    "def check_missing_images(df: pd.DataFrame, image_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"Verifica qu√© im√°genes no se encuentran en el directorio.\"\"\"\n",
    "    missing = []\n",
    "    for idx, row in df.iterrows():\n",
    "        image_filename = row.get(config.IMAGE_COLUMN, '')\n",
    "        if image_filename:\n",
    "            image_path = image_dir / image_filename\n",
    "            if not image_path.exists():\n",
    "                missing.append({\n",
    "                    'id': row.get(config.ID_COLUMN, idx),\n",
    "                    'image': image_filename\n",
    "                })\n",
    "    \n",
    "    missing_df = pd.DataFrame(missing)\n",
    "    print(f\"\\nüîç Im√°genes faltantes: {len(missing)}\")\n",
    "    return missing_df\n",
    "\n",
    "# Descomentar para verificar im√°genes faltantes\n",
    "# missing_images = check_missing_images(df_result, config.IMAGE_DIRECTORY)\n",
    "# missing_images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matriz-atributos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
